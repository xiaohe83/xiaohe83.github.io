---
title: 多云之间数据同步
author: xiao he
date: 2023-06-20
category: Jekyll
layout: post
---

在很多场景中都会有数据同步，比如：多云的CDN静态资源同步；AI计算多集群间源数据同步等。按照技术方案的不同，可以分为如下几类: 一次性同步， 增量实时同步以及跨云增量实时同步。

### 一次性同步
很多的云存储都有存储辅助工具，支持存储桶之间的copy操作， 比如aws s3 copy s等等，这是最简单的方式。如果涉及到跨云， 云厂商也会有对应的服务或者工具：

1. 比如从 aws s3 到 google cloud storage同步数据, 可以使用[google的storage transfer service](https://cloud.google.com/blog/topics/developers-practitioners/transfer-data-aws-gcp-using-storage-transfer-service)
2. 比如从 aws s3 到 aliyun OSS同步数据, 可以使用aliyun 的[Data migration service](https://help.aliyun.com/document_detail/412201.html)
3. 当然aws也有类似的服务， 叫做[Data Transfer Hub](https://aws.amazon.com/cn/solutions/implementations/data-transfer-hub/)
这些的服务比较类似，但能力会有差别，google/aws的迁移工具就可以支持event driven， 来实现近实时的数据迁移

### 增量实时同步
在同一云服务中， 绝大多数云服务都支持存储桶的同区域或者跨区域复制， 可以将数据实时同步到其他区域的其他桶中

### 跨云的增量实时同步
在上面提到， 有些云存储的同步服务能够支持event driven，来实现近实时的数据驱动， 比如google storage transfer service 和 aws的 data transfer hub;
如果云服务本身不支持，或者不想使用云服务的近实时迁移产生额外成本，那么可以自己通过event driven的方式来实现, 在这里介绍3个case:
1. 通过lambda实时copy aws us数据到 aws cn
2. 通过阿里云MNS实时同步OSS数据到 aws
3. 通过rclone来实时同步aws数据到阿里云OSS

#### 通过lambda实时copy aws global数据到 aws cn
首先aws global和aws cn之间独立，不能直接设置桶之间的复制，但可以通过设置桶的event notification来实现近实时同步
1. 在aws global创建源数据桶
![create source bucket](https://pics.gcs.hexhxiao.com/cross-cloud-replication-case-1/aws-create-source-bucket.png?Expires=1790000000&KeyName=signkey&Signature=q9-D_FN8hh2ny5hMEz-Bx9LMcNg=)
2. 在aws cn创建目标数据桶
![create target bucket](https://pics.gcs.hexhxiao.com/cross-cloud-replication-case-1/aws-create-target-bucket.png?Expires=1790000000&KeyName=signkey&Signature=kw2ddguLoGIMYAxduDmUlIk4Hso=)
3. 在aws cn创建IAM，读写目标数据痛
略
4. 在aws global创建桶的event notification
create event notification
![create event notification](https://pics.gcs.hexhxiao.com/cross-cloud-replication-case-1/aws-create-event-notification.png?Expires=1790000000&KeyName=signkey&Signature=j4IPBfn5cZggxh0dBuMIwzk4YSg=)
select event type
![select event type](https://pics.gcs.hexhxiao.com/cross-cloud-replication-case-1/aws-select-event-type.png?Expires=1790000000&KeyName=signkey&Signature=4SqDVTwqUWNHohIZrOBNNtQrEVg=)
select lambda
![select lambda](https://pics.gcs.hexhxiao.com/cross-cloud-replication-case-1/aws-select-lambda.png?Expires=1790000000&KeyName=signkey&Signature=L_XEkf7j6Lo6KPMR89jSP0SWLG8=)
5. 创建lambda函数
``` python
import boto3
def lambda_handler(event, context):
    # Get the source bucket and object key from the event
    source_bucket = event['Records'][0]['s3']['bucket']['name']
    source_object_key = event['Records'][0]['s3']['object']['key']
    # Destination bucket and AWS account ID where you want to replicate the object
    destination_bucket = 'hex-demostration-target-bucket'
    destination_access_key = 'AKXXXX'
    destination_secret_key = 'SKYYYY'
    destination_endpoint = 'https://s3.cn-north-1.amazonaws.com.cn'  
    s3_remote = boto3.client('s3',
                             aws_access_key_id=destination_access_key,
                             aws_secret_access_key=destination_secret_key,
                             endpoint_url=destination_endpoint)
    s3_local = boto3.client('s3')
    # download file from S3 to local
    local_file = "/tmp/" + source_object_key
    s3_local.download_file(source_bucket, source_object_key, local_file)  
    file_info = s3_local.get_object(Bucket = source_bucket, Key = source_object_key)
    if "ContentType" in file_info:
        ExtraARGS = {'ContentType': file_info["ContentType"]}
    else:
        ExtraARGS = {} 
    # upload local file to cn s3
    for i in range(3):
        try:
            s3_remote.upload_file(local_file, destination_bucket, source_object_key, ExtraArgs = ExtraARGS)
        except:
            continue
        break
    return {
        'statusCode': 200,
        'body': 'Object replicated successfully!'
    }
```

6. 需要注意
    1. lambda脚本使用了download_file/upload_file的方式， boto3有copy_object的函数，但因为aws global和aws cn是独立的，所以没办法使用
    2. upload之前需要指定content-type
    3. 注意lambda函数的超时时间， 默认为3s， 如果要同步大文件的话，需要将超时改长一些
    4. 脚本中默认2次失败重试，如果同步关键文件， 对失败重传需要做额外处理


#### 通过阿里云MNS实时同步OSS数据到 aws
和上面aws的lambda类似。阿里云MNS（Message Service）中可以直接创建队列，主题，以及事件通知。 在事件通知中，能够绑定具体的存储OSS桶，然后通过规则过滤事件，发送到队列中。 然后自行写程序来同步内容。


#### 通过rclone来实时同步aws数据到阿里云OSS
rclone是一个管理云存储的开源工具，可以在阿里云运行rclone，然后通过http接口来向rclone发送消息， rclone会读取消息来拉取aws上面的文件，保存到阿里云。本质上和上面的阿里云MNS类似。
